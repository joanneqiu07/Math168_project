{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl \n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import dgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    indices_array = np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64)\n",
    "    # print(indices_array.shape, type(indices_array))\n",
    "\n",
    "    if (indices_array.shape[1] > 0):\n",
    "        indices = torch.LongTensor(indices_array)\n",
    "        values = torch.FloatTensor(sparse_mx.data)\n",
    "        sparse_tensor = torch.sparse.FloatTensor(indices, values, shape)\n",
    "    else:\n",
    "        sparse_tensor = torch.sparse.FloatTensor(shape[0], shape[1])\n",
    "    return sparse_tensor\n",
    "\n",
    "\n",
    "def load_data(args, dirname, use_cuda, SUPERVISE_FLAG):\n",
    "\n",
    "    # Load Data\n",
    "    DATASET = args['dataset']\n",
    "    with open(dirname + '/data/' + DATASET + '.pickle', 'rb') as f:\n",
    "        raw_data = pkl.load(f)\n",
    "    A = raw_data['A']\n",
    "    y = raw_data['y']\n",
    "    idx_train = raw_data['train_idx']\n",
    "    idx_valid = raw_data['valid_idx']\n",
    "    idx_test = raw_data['test_idx']\n",
    "    all_labels = raw_data['all_labels']\n",
    "    all_poli_users = raw_data['all_followees']\n",
    "    all_share_users = raw_data['all_nodes']\n",
    "    all_docs = raw_data['all_docs']\n",
    "    num_docs = len(all_docs)\n",
    "    num_poli_users = len(all_poli_users)\n",
    "    y = np.array(y.todense())\n",
    "    labels = np.argmax(y, axis=1)\n",
    "    num_nodes = A[0].shape[0]\n",
    "    num_non_docs = num_nodes - num_docs\n",
    "    support = len(A)\n",
    "    data = {}\n",
    "\n",
    "    # # test with reduced network\n",
    "    # with open(dirname + '/newsbias_random_1_untyped_40.pickle', 'rb') as f:\n",
    "    #     data_reduced = pkl.load(f)\n",
    "    # A = data_reduced['A']\n",
    "\n",
    "    # only use the labels of political users at training time in the distant supervision case ('unsup1')\n",
    "    if (SUPERVISE_FLAG != 'supervise'):\n",
    "        idx_train = raw_data['train_idx'][:num_poli_users]\n",
    "        idx_test = np.concatenate(\n",
    "            (raw_data['train_idx'][num_poli_users:], raw_data['test_idx']))\n",
    "    # after training only with labels of political users, use predicted labels of articles to train again ('unsup2')\n",
    "    idx_train_set, idx_test_set = set(idx_train), set(idx_test)\n",
    "    if (SUPERVISE_FLAG == 'unsup2'):\n",
    "        idx_train_set = set(idx_test) | set(idx_valid)\n",
    "        fin = open('temp/%s_unsup_pred.pickle' % DATASET, 'rb')\n",
    "        label_preds = pkl.load(fin)\n",
    "        data['label_preds'] = label_preds\n",
    "        print(label_preds.shape)\n",
    "        fin.close()\n",
    "\n",
    "    # print(len(A), len(all_labels), len(all_poli_users), len(all_share_users), len(all_docs))\n",
    "    # print(len(idx_train), len(idx_valid), len(idx_test))\n",
    "\n",
    "    # Define one-hot dummy feature matrix\n",
    "    X = sp.eye(num_nodes).tocsr()\n",
    "    # Normalize adjacency matrices individually\n",
    "    for i in range(len(A)):\n",
    "        d = np.array(A[i].sum(1)).flatten()\n",
    "        d_inv = 1. / d\n",
    "        d_inv[np.isinf(d_inv)] = 0.\n",
    "        D_inv = sp.diags(d_inv)\n",
    "        A[i] = D_inv.dot(A[i]).tocsr()\n",
    "\n",
    "    inputs = [sparse_mx_to_torch_sparse_tensor(item) for item in [X] + A]\n",
    "    # print(len(inputs))\n",
    "    # for input in inputs:\n",
    "    #     print(input.size())\n",
    "    labels_train = torch.LongTensor(labels[idx_train])\n",
    "    labels_valid = torch.LongTensor(labels[idx_valid])\n",
    "    labels_test = torch.LongTensor(labels[idx_test])\n",
    "\n",
    "    if (use_cuda):\n",
    "        inputs = [item.cuda() for item in inputs]\n",
    "        labels_train = labels_train.cuda()\n",
    "        labels_valid = labels_valid.cuda()\n",
    "        labels_test = labels_test.cuda()\n",
    "\n",
    "    x_pos, y_pos = A[0].tocoo().row, A[0].tocoo().col\n",
    "    all_docs_set, all_shareu_set = set(all_docs), set(all_share_users)\n",
    "    node2adj = {}\n",
    "    for xi, yi in zip(x_pos, y_pos):\n",
    "        if (xi not in all_docs_set or yi not in all_shareu_set):\n",
    "            continue\n",
    "        if xi not in node2adj:\n",
    "            node2adj[xi] = []\n",
    "        node2adj[xi].append(yi)\n",
    "    num_edges_list, num_labels_list = [], [0, 0, 0]\n",
    "    for node in idx_test:\n",
    "        if node not in node2adj:\n",
    "            num_edges_list.append(0)\n",
    "        else:\n",
    "            num_edges_list.append(len(node2adj[node]))\n",
    "        num_labels_list[labels[node]] += 1\n",
    "    # print(len(num_edges_list), sum(num_edges_list)/len(num_edges_list))\n",
    "    # print(sum(num_labels_list), np.array(num_labels_list)/sum(num_labels_list))\n",
    "    # print(len(node2adj))\n",
    "\n",
    "    data['idx_train'] = idx_train\n",
    "    data['idx_valid'] = idx_valid\n",
    "    data['idx_test'] = idx_test\n",
    "    data['idx_train_set'] = idx_train_set\n",
    "    data['idx_test_set'] = idx_test_set\n",
    "    data['inputs'] = inputs\n",
    "    data['labels_train'] = labels_train\n",
    "    data['labels_valid'] = labels_valid\n",
    "    data['labels_test'] = labels_test\n",
    "    data['num_nodes'] = num_nodes\n",
    "    data['num_docs'] = num_docs\n",
    "    data['num_non_docs'] = num_non_docs\n",
    "    data['node2adj'] = node2adj\n",
    "    data['support'] = support\n",
    "\n",
    "    return raw_data, data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7c/t8cb08nj5hq59l66ln0b1bq40000gn/T/ipykernel_36335/3193289881.py:22: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  raw_data = pkl.load(f)\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    'dataset': 'newsbias_event_1_untyped'\n",
    "}\n",
    "\n",
    "raw_data, data = load_data(args, '.', False, \"unsup1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['idx_train', 'idx_valid', 'idx_test', 'idx_train_set', 'idx_test_set', 'inputs', 'labels_train', 'labels_valid', 'labels_test', 'num_nodes', 'num_docs', 'num_non_docs', 'node2adj', 'support'])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = data['num_nodes']\n",
    "srcs = []\n",
    "dests = []\n",
    "for src in data['node2adj']:\n",
    "    for dest in data['node2adj'][src]:\n",
    "        srcs.append(src)\n",
    "        dests.append(dest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(idxs, N):\n",
    "    result = [0] * N\n",
    "    for i in idxs:\n",
    "        result[i] = 1\n",
    "    return torch.LongTensor(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = dgl.graph(\n",
    "    (srcs, dests), num_nodes = N\n",
    ")\n",
    "graph.ndata[\"label\"] = torch.LongTensor(data[''])\n",
    "graph.ndata[\"train_mask\"] = create_mask(data['idx_train'], N)\n",
    "graph.ndata[\"val_mask\"] = create_mask(data['idx_valid'], N)\n",
    "graph.ndata[\"test_mask\"] = create_mask(data['idx_test'], N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10520"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12127"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
